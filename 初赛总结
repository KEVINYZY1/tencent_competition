1、A阶段初期做了一些简单的特征工程，提升效果一般，后面尝试了将兴趣类特征训练embedding然后和原始特征组合后重新训练，有一定的提升；再者尝试了2014年kaggle
CTR比赛冠军的做法，训练了一个只有30 estimator，10个leaves的lgb，然后将lgb产生的叶子结点作为新的特征，与原始特征进行组合后重新训练，本地测试效果很好
但是线上效果一般，能达到做了特征工程后的效果，但容易过拟合
2、尝试了用hyperopt进行调参，太耗费时间，estimator的大小不好把控，再者尝试在lgb中加入scale_pos_weight参数，得出最好的结果是3，线下效果很好，线上效果
一般，最终仍决定采用baseline的参数
3、用xlearn尝试了ffm，但效果始终不理想，可能数据预处理做的不够好，其次尝试了深度学习模型deepfm/deepffm，deepfm线上只有0.70，deepffm训练有点慢没来得及
提交
4、B阶段只是简单的跑了一下之前的流程，然后最后阶段交给队友进行融合，融合效果不错，最终提升有5个多千分点，融合方式采用的sigmoid反函数，然后加权平均
5、总结来讲特征工程方面做得太差，都是前期靠人工一个个筛选，效率低而且结果稳定性太差，其次深度学习模型效果一直不理想。
